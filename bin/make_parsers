#!/usr/bin/env ruby

require "openai"
require "json"
require "nokogiri"
require "dotenv/load"
require "fileutils"

if ARGV.size != 2
  puts "Usage: #{$0} {output_dir} {parsers_dir}"
  exit 1
end

CLIENT = OpenAI::Client.new(
  access_token: ENV["OPENAI_API_KEY"],
  organization_id: ENV["OPENAI_ORGANIZATION_ID"],
  log_errors: true,
  request_timeout: 240,
)

def initial_prompt(url:, html:)
  <<~EOF
    Given the following HTML:

    ```
    #{html}
    ```

    downloaded from URL: #{url}

    Identify the job listings in the HTML and return:

    - The job title
    - The job description (including relevant details such as department, application deadline, hours/week, etc.)
    - The URL to the job posting, which should be an absolute URL
    ```
  EOF
end

def write_code_prompt
  <<~EOF
    Write a ruby function, that takes a HTML string (like the one provided previously) as input, parses it using `nokogiri` (which you can assume is installed and already required),
    and returns the job postings from the markup as a list of hashes with the following keys: `:name`, `:description`, `:url`.

    Make sure all the listings you returned previously are included in the output.

    DO NOT include any `require` statements or anything besides the `parse_job_postings` method. The returned code must start with `def parse_job_postings(html)` and end with `end`.
    DO NOT use any complex XPath selectors. Prefer iterating over the elements and simple checks in ruby.
    DO NOT include any instructions about how to use the code. Just the `parse_job_postings` method. Nothing else.
    If you include example usage, you fail the assignment.

    ```ruby
    def parse_job_postings(html)
      # implement this
    end
    ```
  EOF
end

def update_code_prompt(output:)
  <<~EOF
    The code you provided produces the following output for the given HTML:

    ```
    #{output}
    ```

    First, describe what the code does correctly and what it does incorrectly.

    Make sure that the number of listings returned is correct.
    Make sure the code correctly extracts the job name and description, and that all the URLs are valid.
    Make sure all the URLs are absolute and not relative.
    Filter out results that are not actual job postings.

    Please update the code and provide the corrected version based on your critique.

    DO NOT include any instructions about how to use the code. Just the `parse_job_postings` method. Nothing else.
    If you include example usage, you fail the assignment.
  EOF
end

def list_job_postings(url:, html:)
  response = CLIENT.chat(
    parameters: {
      model: "gpt-4o",
      temperature: 0.0,
      messages: [{ role: "user", content: initial_prompt(url: url, html: html) }],
    }
  )

  response.dig("choices", 0, "message", "content")
end

def make_parser(url:, html:, job_postings_response:)
  response = CLIENT.chat(
    parameters: {
      model: "gpt-4o",
      temperature: 0.0,
      messages: [
        { role: "user", content: initial_prompt(url: url, html: html) },
        { role: "assistant", content: job_postings_response },
        { role: "user", content: write_code_prompt },
      ],
    }
  )

  response.dig("choices", 0, "message", "content")
end

def update_parser(url:, html:, results:)
  results = results.flat_map do |r|
    [
      { role: "assistant", content: r.fetch(:model_response) },
      { role: "user", content: update_code_prompt(output: r.fetch(:output)) },
    ]
  end

  response = CLIENT.chat(
    parameters: {
      model: "gpt-4o",
      temperature: 0.0,
      messages: [
        { role: "user", content: initial_prompt(url:, html:) },
        *results,
      ],
    }
  )

  response.dig("choices", 0, "message", "content")
end

def extract_ruby_method(response)
  ruby_block = response.match(/```ruby\n(.+?)\n```/m)[1]
  ruby_block.gsub(/^require.+\n/, "")
end

def test_code(code:, html:)
  parser = Object.new
  parser.instance_eval(code)

  begin
    output = parser.parse_job_postings(html).inspect
  rescue => e
    output = "#{e.class}: #{e.message}\n#{e.backtrace.join("\n")}"
  end
  puts output

  output
end

output_dir = File.expand_path(ARGV[0])
parsers_dir = File.expand_path(ARGV[1])

metadata_file = "metadata.json"
metadata = JSON.parse(File.read(metadata_file), symbolize_names: true)

FileUtils.mkdir_p(parsers_dir)

metadata.each do |url, meta|
  tries = 3

  begin
    out_file = File.join(output_dir, "#{meta[:idx]}.html")
    next unless File.exist?(out_file)

    parser_file = File.join(parsers_dir, "#{meta[:idx]}.rb")
    next if File.exist?(parser_file)

    puts "Generating parser for #{url}..."

    html = File.read(out_file)
    results = []

    job_postings_response = list_job_postings(url:, html:)
    puts "Job postings for #{url}:\n\n #{job_postings_response}"

    response = make_parser(url:, html:, job_postings_response:)
    code = extract_ruby_method(response)

    File.write(parser_file, code)

    output = test_code(code:, html:)
    results << { model_response: response, output: output }

    response = update_parser(url:, html:, results:)
    code = extract_ruby_method(response)
    File.write(parser_file, code)
    output = test_code(code:, html:)
    results << { model_response: response, output: output }

    response = update_parser(url:, html:, results:)
    code = extract_ruby_method(response)
    File.write(parser_file, code)
    output = test_code(code:, html:)

    if output.include?("Error") || output == "[]"
      fail "Failed to generate parser for #{url}, last output: #{output}"
    end
  rescue StandardError => e
    puts e
    puts e.backtrace
    tries -= 1

    retry if tries >= 0
  end
end